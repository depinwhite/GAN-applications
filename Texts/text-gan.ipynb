{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.7\n",
    "!pip install torch==1.6\n",
    "!pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575183c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.9 torch\n",
    "# 0.10 torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a64135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import mlflow\n",
    "import torch\n",
    "import os\n",
    "from torch.autograd import Variable, grad\n",
    "from torch import optim\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import IMDB\n",
    "from mlflow import log_metric, log_param\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d753a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(dim, dim, 5, padding=2),  # nn.Linear(DIM, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(dim, dim, 5, padding=2),  # nn.Linear(DIM, DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.res_block(input)\n",
    "        return input + (0.3 * output)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, dim, seq_len, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.fc1 = nn.Linear(128, dim * seq_len)\n",
    "        self.block = nn.Sequential(\n",
    "            ResBlock(dim),\n",
    "            ResBlock(dim),\n",
    "            ResBlock(dim),\n",
    "            ResBlock(dim),\n",
    "            ResBlock(dim),\n",
    "        )\n",
    "        self.conv1 = nn.Conv1d(dim, vocab_size, 1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        batch_size = noise.size(0)\n",
    "        output = self.fc1(noise)\n",
    "        # (BATCH_SIZE, DIM, SEQ_LEN)\n",
    "        output = output.view(-1, self.dim, self.seq_len)\n",
    "        output = self.block(output)\n",
    "        output = self.conv1(output)\n",
    "        output = output.transpose(1, 2)\n",
    "        shape = output.size()\n",
    "        output = output.contiguous()\n",
    "        output = output.view(batch_size * self.seq_len, -1)\n",
    "        output = self.softmax(output)\n",
    "        # (BATCH_SIZE, SEQ_LEN, len(charmap))\n",
    "        output = output.view(shape)\n",
    "        return output.view(shape)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, dim, seq_len, vocab_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ResBlock(dim),\n",
    "            ResBlock(dim),\n",
    "            ResBlock(dim),\n",
    "            ResBlock(dim),\n",
    "            ResBlock(dim),\n",
    "        )\n",
    "        self.conv1d = nn.Conv1d(vocab_size, dim, 1)\n",
    "        self.linear = nn.Linear(seq_len * dim, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # (BATCH_SIZE, VOCAB_SIZE, SEQ_LEN)\n",
    "        output = input.transpose(1, 2)\n",
    "        output = self.conv1d(output)\n",
    "        output = self.block(output)\n",
    "        output = output.view(-1, self.seq_len * self.dim)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161974f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalize_grad(D, real, fake, batch_size, lamb, use_cuda=True):\n",
    "    \"\"\"\n",
    "    lamb: lambda\n",
    "    \"\"\"\n",
    "    alpha = torch.rand(batch_size, 1, 1).expand(real.size())\n",
    "    if use_cuda:\n",
    "        alpha = alpha.cuda()\n",
    "    interpolates = alpha * real + ((1 - alpha) * fake)\n",
    "    if use_cuda:\n",
    "        interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    ones = torch.ones(d_interpolates.size())\n",
    "    if use_cuda:\n",
    "        ones = ones.cuda()\n",
    "    gradients = grad(outputs=d_interpolates, inputs=interpolates,\n",
    "                     grad_outputs=ones, create_graph=True,\n",
    "                     retain_graph=True, only_inputs=True)[0]\n",
    "    grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lamb\n",
    "    return grad_penalty\n",
    "\n",
    "\n",
    "def train_discriminator(D, G, optim_D, real, lamb, batch_size, use_cuda=True):\n",
    "    D.zero_grad()\n",
    "\n",
    "    # train with real\n",
    "    d_real = D(real)\n",
    "    d_real = d_real.mean()\n",
    "    d_real.backward(mone)\n",
    "\n",
    "    # train with fake\n",
    "    noise = torch.randn(batch_size, 128)\n",
    "    if use_cuda:\n",
    "        noise = noise.cuda()\n",
    "    noise = noise  # freeze G\n",
    "    fake = G(noise)\n",
    "    fake = Variable(fake.data)\n",
    "    inputv = fake\n",
    "    d_fake = D(inputv)\n",
    "    d_fake = d_fake.mean()\n",
    "    d_fake.backward(one)\n",
    "\n",
    "    grad_penalty = penalize_grad(D, real.data, fake.data,\n",
    "                                 batch_size, lamb, use_cuda)\n",
    "    grad_penalty.backward()\n",
    "\n",
    "    d_loss = d_fake - d_real + grad_penalty\n",
    "    wasserstein = d_real - d_fake\n",
    "    optim_D.step()\n",
    "    return d_loss, wasserstein\n",
    "\n",
    "\n",
    "def train_generator(D, G, optim_G, batch_size, use_cuda=True):\n",
    "    G.zero_grad()\n",
    "    noise = torch.randn(batch_size, 128)\n",
    "    if use_cuda:\n",
    "        noise = noise.cuda()\n",
    "    noisev = Variable(noise)\n",
    "    fake = G(noisev)\n",
    "    g = D(fake)\n",
    "    g = g.mean()\n",
    "    g.backward(mone)\n",
    "    g_loss = -g\n",
    "    optim_G.step()\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab9a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(index, vocab_size):\n",
    "    batch_size, seq_len = index.size(0), index.size(1)\n",
    "    onehot = torch.FloatTensor(batch_size, seq_len, vocab_size).zero_()\n",
    "    onehot.scatter_(2, index.data.cpu().unsqueeze(2), 1)\n",
    "    return onehot\n",
    "\n",
    "\n",
    "def sample(G, TEXT, batch_size, seq_len, vocab_size, use_cuda=True):\n",
    "    noise = torch.randn(batch_size, 128)\n",
    "    if use_cuda:\n",
    "        noise = noise.cuda()\n",
    "    noisev = noise\n",
    "    with torch.no_grad():\n",
    "        samples = G(noisev)\n",
    "    samples = samples.view(-1, seq_len, vocab_size)\n",
    "    _, argmax = torch.max(samples, 2)\n",
    "    argmax = argmax.cpu().data\n",
    "    decoded_samples = []\n",
    "    for i in range(len(argmax)):\n",
    "        decoded = \"\".join([TEXT.vocab.itos[s] for s in argmax[i]])\n",
    "        decoded_samples.append(decoded)\n",
    "    return decoded_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchs=500000\n",
    "critic_iters=5\n",
    "batch_size=8\n",
    "seq_len=280\n",
    "lamb=10\n",
    "lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82834ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "# load datasets\n",
    "\n",
    "print(\"[!] preparing dataset...\")\n",
    "TEXT = Field(lower=True, fix_length=seq_len,\n",
    "             tokenize=list, batch_first=True)\n",
    "LABEL = Field(sequential=False)\n",
    "train_data, test_data = IMDB.splits(TEXT, LABEL)\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "        (train_data, test_data), batch_size=batch_size, repeat=True)\n",
    "vocab_size = len(TEXT.vocab)\n",
    "print(\"[TRAIN]:%d (dataset:%d)\\t[TEST]:%d (dataset:%d)\\t[VOCAB]:%d\"\n",
    "      % (len(train_iter), len(train_iter.dataset),\n",
    "         len(test_iter), len(test_iter.dataset), vocab_size))\n",
    "\n",
    "# instantiate models\n",
    "G = Generator(dim=512, seq_len=seq_len, vocab_size=vocab_size)\n",
    "D = Discriminator(dim=512, seq_len=seq_len, vocab_size=vocab_size)\n",
    "optim_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optim_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "global one, mone\n",
    "one = torch.tensor(1, dtype=torch.float)\n",
    "mone = one * -1\n",
    "if use_cuda:\n",
    "    G, D = G.cuda(), D.cuda()\n",
    "    one, mone = one.cuda(), mone.cuda()\n",
    "\n",
    "train_iter = iter(train_iter)\n",
    "batch_size = batch_size\n",
    "\n",
    "with mlflow.start_run():\n",
    "    log_param(\"batch_size\", batch_size)\n",
    "    log_param(\"initial_lr\", lr)\n",
    "    log_param(\"max_epochs\", batchs)\n",
    "    for b in tqdm(range(1, batchs+1)):\n",
    "        # (1) Update D network\n",
    "        for p in D.parameters():  # reset requires_grad\n",
    "            p.requires_grad = True\n",
    "        for iter_d in range(critic_iters):  # CRITIC_ITERS\n",
    "            batch = next(train_iter)\n",
    "            text, label = batch.text, batch.label\n",
    "            text = to_onehot(text, vocab_size)\n",
    "            if use_cuda:\n",
    "                text = text.cuda()\n",
    "            real = text\n",
    "            d_loss, wasserstein = train_discriminator(\n",
    "                    D, G, optim_D, real, lamb, batch_size, use_cuda)\n",
    "        # (2) Update G network\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = False  # to avoid computation\n",
    "        g_loss = train_generator(D, G, optim_G, batch_size, use_cuda)\n",
    "\n",
    "        try:\n",
    "            mlflow.log_metric('Discriminator Loss', d_loss.data.item())\n",
    "            mlflow.log_metric('Generator Loss', g_loss.data.item())\n",
    "            mlflow.log_metric('Wasserstein distance', wasserstein.data.item())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        samples = sample(G, TEXT, 1, seq_len, vocab_size, use_cuda)\n",
    "\n",
    "        if b % 500 == 0 and b > 1:\n",
    "            samples = sample(G, TEXT, 1, seq_len, vocab_size, use_cuda)\n",
    "            with open('text_samples.txt', 'a') as f:\n",
    "                f.write(f' ------------------------------------------------- \\n epoch #{b}  D:{d_loss.data.item()}  G:{g_loss.data.item()}  W:{wasserstein.data.item()}\\n sample: {samples[0]}\\n ------------------------------------------------- \\n')\n",
    "\n",
    "        if b % 5000 == 0 and b > 1:\n",
    "            print(\"[!] saving model\")\n",
    "            if not os.path.isdir(\".save\"):\n",
    "                os.makedirs(\".save\")\n",
    "            torch.save(G.state_dict(), './.save/wgan_g_%d.pt' % (b))\n",
    "            torch.save(D.state_dict(), './.save/wgan_d_%d.pt' % (b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
